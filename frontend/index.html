<!DOCTYPE html>
<html lang="es">
<meta charset="UTF-8" />
<!-- Encabezado alineado a la izquierda -->
<div class="header">
  <h1>ğŸ™ï¸ REading</h1>
</div>

<!-- ExplicaciÃ³n y selector de nivel -->
<div class="level-row">
  <p class="instructions">Selecciona un nivel, lee el texto en voz alta y luego envÃ­a el audio para analizarlo.</p>
  <div class="level-selector">
    <label for="levelSelect">ğŸ“š Nivel de lectura:</label>
    <select id="levelSelect">
      <option value="easy">Nivel 1 â€“ FÃ¡cil</option>
      <option value="medium">Nivel 2 â€“ Intermedio</option>
      <option value="hard">Nivel 3 â€“ Avanzado</option>
    </select>
  </div>
</div>

<!-- Contenedor principal -->
<div class="reading-visual-container">
  <!-- Texto a leer -->
  <div id="readingText"></div>

  <!-- CÃ¡mara y canvas -->
  <div class="video-container">
    <video id="video" autoplay muted playsinline width="320" height="240"></video>
    <canvas id="output" width="320" height="240"></canvas>
  </div>

  <!-- DirecciÃ³n de la mirada -->
  <p id="gazeStatus">ğŸ‘ï¸ DirecciÃ³n de la mirada: desconocida</p>
</div>


  <style>
body {
  font-family: Arial, sans-serif;
  margin: 0;
  padding: 20px;
  display: flex;
  flex-direction: column;
  align-items: center;
  text-align: left;
}

.header {
  width: 100%;
  margin-bottom: 10px;
}

.level-row {
  display: flex;
  flex-wrap: wrap;
  justify-content: space-between;
  align-items: center;
  gap: 20px;
  max-width: 800px;
  margin-bottom: 20px;
}

.instructions {
  flex: 1;
  font-size: 16px;
  margin: 0;
}

.level-selector {
  display: flex;
  align-items: center;
  gap: 10px;
}

#readingText {
  background-color: #fefefe;
  border: 2px solid #ccc;
  padding: 16px;
  font-size: 30px;
  line-height: 1.6;
  max-width: 600px;
  width: 100%;
  margin: 20px auto;
  box-shadow: 0 0 8px rgba(0,0,0,0.1);
  text-align: left;
}

.reading-visual-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 20px;
  margin-top: 20px;
}

.controls {
  position: absolute;
  left: 20px;
  top: 40px;
  display: flex;
  flex-direction: column;
  gap: 10px;
}

audio {
  margin-top: 10px;
}

.video-container {
  position: relative;
  width: 320px;
  height: 240px;
  margin-top: 20px;
}

#video {
  border: 1px solid #ccc;
}

#output {
  position: absolute;
  left: 0;
  top: 0;
  width: 320px;
  height: 240px;
  pointer-events: none;
  background-color: transparent;
}

@media (max-width: 600px) {
  .video-container {
    width: 100%;
    height: auto;
  }

  #video, #output {
    width: 100%;
    height: auto;
  }

  #readingText {
    font-size: 18px;
  }
}
  </style>
</head>
<body>
  <body>
  <h1>ğŸ™ï¸ Reading360</h1>
  <!-- Selector de nivel -->
 <div class="level-selector">
  <span>Selecciona un nivel, lee el texto en voz alta y luego envÃ­a el audio para analizarlo. ğŸ“š Nivel de lectura:</span>
  <select id="levelSelect">
    <option value="easy">Nivel 1 â€“ FÃ¡cil</option>
    <option value="medium">Nivel 2 â€“ Intermedio</option>
    <option value="hard">Nivel 3 â€“ Avanzado</option>
  </select>
</div>

  <!-- Contenedor principal: texto + cÃ¡mara -->
 <div class="reading-visual-container">
  <div id="readingText"></div>

  <div class="video-container">
    <video id="video" autoplay muted playsinline width="320" height="240"></video>
    <canvas id="output" width="320" height="240"></canvas>
  </div>

  <p id="gazeStatus">ğŸ‘ï¸ DirecciÃ³n de la mirada: desconocida</p>
</div>


  <!-- Botones de grabaciÃ³n en lateral -->
  <div class="controls">
    <button id="startBtn">ğŸ”´ Grabar</button>
    <button id="stopBtn" disabled>â¹ï¸ Detener</button>
    <button id="sendBtn" disabled>ğŸ“¤ Enviar</button>
  </div>

  <audio id="audioPlayback" controls></audio>

  <!-- Scripts MediaPipe -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

  <script>
    // Textos por nivel
    const texts = {
      easy: "El gato duerme en la cama. La niÃ±a lee un libro. El sol brilla en el cielo.",
      medium: "Cada maÃ±ana, Marcos se levanta temprano para ir al colegio. Se lava la cara, desayuna con su familia y sale con su mochila llena de libros.",
      hard: "Aunque el cielo estaba cubierto de nubes, Clara decidiÃ³ salir a caminar por el bosque. El aire era fresco, las hojas crujÃ­an bajo sus pies y el silencio la envolvÃ­a como una manta suave."
    };

    const levelSelect = document.getElementById("levelSelect");
    const readingText = document.getElementById("readingText");
    readingText.innerText = texts[levelSelect.value];
    levelSelect.onchange = () => {
      readingText.innerText = texts[levelSelect.value];
    };

    // GrabaciÃ³n de voz
    let mediaRecorder;
    let audioChunks = [];
    let latestVoiceMetrics = null;
    let latestVisualMetrics = null;
    let readingRegistered = false;
    const sessionId = Date.now().toString();

    const startBtn = document.getElementById("startBtn");
    const stopBtn = document.getElementById("stopBtn");
    const sendBtn = document.getElementById("sendBtn");
    const audioPlayback = document.getElementById("audioPlayback");

    startBtn.onclick = async () => {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      audioChunks = [];

      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
        audioPlayback.src = URL.createObjectURL(audioBlob);
        sendBtn.disabled = false;
      };

      mediaRecorder.start();
      startBtn.disabled = true;
      stopBtn.disabled = false;
    };

    stopBtn.onclick = () => {
      mediaRecorder.stop();
      startBtn.disabled = false;
      stopBtn.disabled = true;
    };

    sendBtn.onclick = async () => {
      const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
      const formData = new FormData();
      formData.append("audio", audioBlob, "lectura.wav");

      const response = await fetch("http://localhost:5000/upload-audio", {
        method: "POST",
        body: formData
      });

      const voiceResult = await response.json();
      latestVoiceMetrics = voiceResult;
      alert("âœ… Voz analizada:\n" + JSON.stringify(voiceResult, null, 2));
      tryRegisterReading();
    };

    async function tryRegisterReading() {
      if (readingRegistered) return;
      if (!latestVoiceMetrics || !latestVisualMetrics) {
        console.log("â³ Esperando ambos anÃ¡lisis para registrar...");
        return;
      }

      const readingData = {
        session_id: sessionId,
        user_id: "demo_user",
        text_id: levelSelect.value,
        label: "unlabeled",
        voice: latestVoiceMetrics,
        visual: latestVisualMetrics
      };

      const res = await fetch("http://localhost:5000/register-reading", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(readingData)
      });

      const result = await res.json();
      alert("ğŸ“š Lectura registrada:\n" + JSON.stringify(result, null, 2));
      readingRegistered = true;
    }

    // Seguimiento facial refinado
    const videoElement = document.getElementById('video');
    const canvasElement = document.getElementById('output');
    const canvasCtx = canvasElement.getContext('2d');

    const faceMesh = new FaceMesh({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
    });

    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });

    const leftEyeIndices = FACEMESH_LEFT_EYE.map(pair => pair[0]);
    const rightEyeIndices = FACEMESH_RIGHT_EYE.map(pair => pair[0]);

    let lastSent = 0;

    function averagePoint(points) {
      const sum = points.reduce((acc, p) => ({
        x: acc.x + p.x,
        y: acc.y + p.y
      }), { x: 0, y: 0 });

      return {
        x: sum.x / points.length,
        y: sum.y / points.length
      };
    }

    faceMesh.onResults(results => {
  const now = Date.now();
  if (now - lastSent < 2000) return;

  canvasCtx.save();
  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
  canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);

  if (results.multiFaceLandmarks) {
    for (const landmarks of results.multiFaceLandmarks) {
      // Dibujar malla facial
      drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION, { color: '#C0C0C0', lineWidth: 1 });
      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYE, { color: '#FF3030' });
      drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_EYE, { color: '#30FF30' });

      // CÃ¡lculo de direcciÃ³n de la mirada usando iris derecho (cuanto mÃ¡s cerca de 0.45 = izquierda, 0.65 = derecha)
      let gazeDirection = "centro";
      try {
        const leftCorner = landmarks[33];   // borde izquierdo del ojo derecho
        const rightCorner = landmarks[133]; // borde derecho del ojo derecho
        const pupil = landmarks[468];       // centro estimado del iris derecho

        const eyeWidth = rightCorner.x - leftCorner.x;
        const pupilOffset = pupil.x - leftCorner.x;
        const ratio = 1 - pupilOffset / eyeWidth;

        if (ratio < 0.35) gazeDirection = "izquierda";
        else if (ratio > 0.55) gazeDirection = "derecha";
        else gazeDirection = "centro"; 
      } catch (e) {
        console.warn("No se pudo calcular la direcciÃ³n de la mirada con el iris.");
      }

      // CÃ¡lculo de centros de ojos (para anÃ¡lisis visual adicional)
      const leftEyePoints = leftEyeIndices.map(i => landmarks[i]);
      const rightEyePoints = rightEyeIndices.map(i => landmarks[i]);

      const leftEyeCenter = averagePoint(leftEyePoints);
      const rightEyeCenter = averagePoint(rightEyePoints);

      // Mostrar direcciÃ³n en pantalla
      document.getElementById("gazeStatus").innerText = "ğŸ‘ï¸ DirecciÃ³n de la mirada: " + gazeDirection;

      // Enviar datos visuales al backend
      fetch("http://localhost:5000/upload-visual", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          leftEyeCenter,
          rightEyeCenter,
          gazeDirection
        })
      })
      .then(res => res.json())
      .then(data => {
        latestVisualMetrics = data;
        console.log("âœ… AtenciÃ³n visual:", data);
        tryRegisterReading();
      })
      .catch(err => console.error("âŒ Error al enviar datos visuales:", err));
    }

    lastSent = now;
  }

  canvasCtx.restore();
});


    const camera = new Camera(videoElement, {
      onFrame: async () => {
        await faceMesh.send({ image: videoElement });
      },
      width: 320,
      height: 240
    });
    camera.start();
  </script>
</body>
</html>
