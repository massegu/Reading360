<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <title>Reading360 - GrabaciÃ³n de voz</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 20px;
      max-width: 800px;
    }

    h1, h2, h3 {
      margin-top: 30px;
    }

    select, button {
      margin: 10px 5px;
      padding: 8px;
      font-size: 16px;
    }

    #readingText {
      background-color: #f9f9f9;
      border: 1px solid #ccc;
      padding: 12px;
      font-size: 22px;
      line-height: 1.5;
      margin-top: 10px;
    }

    audio {
      margin-top: 10px;
    }

    .video-container {
      position: relative;
      width: 320px;
      height: 240px;
      margin-top: 20px;
    }

    #video {
      border: 1px solid #ccc;
    }

    #output {
      position: absolute;
      left: 0;
      top: 0;
    }
  </style>
</head>
<body>
  <h1>ğŸ™ï¸ Reading360</h1>
  <p>Selecciona un nivel, lee el texto en voz alta y luego envÃ­a el audio para analizarlo.</p>

  <!-- Selector de nivel -->
  <label for="levelSelect">ğŸ“š Nivel de lectura:</label>
  <select id="levelSelect">
    <option value="easy">Nivel 1 â€“ FÃ¡cil</option>
    <option value="medium">Nivel 2 â€“ Intermedio</option>
    <option value="hard">Nivel 3 â€“ Avanzado</option>
  </select>

  <!-- Texto dinÃ¡mico -->
  <h3>Texto para leer:</h3>
  <p id="readingText"></p>

  <!-- Botones de grabaciÃ³n -->
  <button id="startBtn">ğŸ”´ Grabar</button>
  <button id="stopBtn" disabled>â¹ï¸ Detener</button>
  <button id="sendBtn" disabled>ğŸ“¤ Enviar</button>

  <audio id="audioPlayback" controls></audio>

  <!-- Seguimiento facial -->
  <h2>ğŸ‘ï¸ Seguimiento facial</h2>
  <div class="video-container">
    <video id="video" autoplay muted playsinline width="320" height="240"></video>
    <canvas id="output" width="320" height="240"></canvas>
  </div>

  <!-- Indicador de direcciÃ³n de la mirada -->
  <p id="gazeStatus">ğŸ‘ï¸ DirecciÃ³n de la mirada: desconocida</p>

  <!-- Scripts MediaPipe -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

  <script>
    // Textos por nivel
    const texts = {
      easy: "El gato duerme en la cama. La niÃ±a lee un libro. El sol brilla en el cielo.",
      medium: "Cada maÃ±ana, Marcos se levanta temprano para ir al colegio. Se lava la cara, desayuna con su familia y sale con su mochila llena de libros.",
      hard: "Aunque el cielo estaba cubierto de nubes, Clara decidiÃ³ salir a caminar por el bosque. El aire era fresco, las hojas crujÃ­an bajo sus pies y el silencio la envolvÃ­a como una manta suave."
    };

    const levelSelect = document.getElementById("levelSelect");
    const readingText = document.getElementById("readingText");
    readingText.innerText = texts[levelSelect.value];
    levelSelect.onchange = () => {
      readingText.innerText = texts[levelSelect.value];
    };

    // GrabaciÃ³n de voz
    let mediaRecorder;
    let audioChunks = [];
    let latestVoiceMetrics = null;
    let latestVisualMetrics = null;
    let readingRegistered = false;
    const sessionId = Date.now().toString();

    const startBtn = document.getElementById("startBtn");
    const stopBtn = document.getElementById("stopBtn");
    const sendBtn = document.getElementById("sendBtn");
    const audioPlayback = document.getElementById("audioPlayback");

    startBtn.onclick = async () => {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      audioChunks = [];

      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
        audioPlayback.src = URL.createObjectURL(audioBlob);
        sendBtn.disabled = false;
      };

      mediaRecorder.start();
      startBtn.disabled = true;
      stopBtn.disabled = false;
    };

    stopBtn.onclick = () => {
      mediaRecorder.stop();
      startBtn.disabled = false;
      stopBtn.disabled = true;
    };

    sendBtn.onclick = async () => {
      const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
      const formData = new FormData();
      formData.append("audio", audioBlob, "lectura.wav");

      const response = await fetch("http://localhost:5000/upload-audio", {
        method: "POST",
        body: formData
      });

      const voiceResult = await response.json();
      latestVoiceMetrics = voiceResult;
      alert("âœ… Voz analizada:\n" + JSON.stringify(voiceResult, null, 2));
      tryRegisterReading();
    };

    async function tryRegisterReading() {
      if (readingRegistered) return;
      if (!latestVoiceMetrics || !latestVisualMetrics) {
        console.log("â³ Esperando ambos anÃ¡lisis para registrar...");
        return;
      }

      const readingData = {
        session_id: sessionId,
        user_id: "demo_user",
        text_id: levelSelect.value,
        label: "unlabeled",
        voice: latestVoiceMetrics,
        visual: latestVisualMetrics
      };

      const res = await fetch("http://localhost:5000/register-reading", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(readingData)
      });

      const result = await res.json();
      alert("ğŸ“š Lectura registrada:\n" + JSON.stringify(result, null, 2));
      readingRegistered = true;
    }

    // Seguimiento facial refinado
    const videoElement = document.getElementById('video');
    const canvasElement = document.getElementById('output');
    const canvasCtx = canvasElement.getContext('2d');

    const faceMesh = new FaceMesh({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
    });

    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });

    const leftEyeIndices = FACEMESH_LEFT_EYE.map(pair => pair[0]);
    const rightEyeIndices = FACEMESH_RIGHT_EYE.map(pair => pair[0]);

    let lastSent = 0;

    function averagePoint(points) {
      const sum = points.reduce((acc, p) => ({
        x: acc.x + p.x,
        y: acc.y + p.y
      }), { x: 0, y: 0 });

      return {
        x: sum.x / points.length,
        y: sum.y / points.length
      };
    }

    faceMesh.onResults(results => {
      const now = Date.now();
      if (now - lastSent < 2000) return;

      canvasCtx.save();
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);

      if (results.multiFaceLandmarks) {
        for (const landmarks of results.multiFaceLandmarks) {
          drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION, { color: '#C0C0C0', lineWidth: 1 });
          drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYE, { color: '#FF3030' });
          drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_EYE, { color: '#30FF30' });

          const leftEyePoints = leftEyeIndices.map(i => landmarks[i]);
          const rightEyePoints = rightEyeIndices.map(i => landmarks[i]);

          const leftEyeCenter = averagePoint(leftEyePoints);
          const rightEyeCenter = averagePoint(rightEyePoints);

          const eyeMidpoint = {
            x: (leftEyeCenter.x + rightEyeCenter.x) / 2,
            y: (leftEyeCenter.y + rightEyeCenter.y) / 2
          
          };

          let gazeDirection = "centro";
          if (eyeMidpoint.x < 0.45) gazeDirection = "izquierda";
          else if (eyeMidpoint.x > 0.55) gazeDirection = "derecha";

          document.getElementById("gazeStatus").innerText = "ğŸ‘ï¸ DirecciÃ³n de la mirada: " + gazeDirection;

          fetch("http://localhost:5000/upload-visual", {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              leftEyeCenter,
              rightEyeCenter,
              gazeDirection
            })
          })
          .then(res => res.json())
          .then(data => {
            latestVisualMetrics = data;
            console.log("âœ… AtenciÃ³n visual:", data);
            tryRegisterReading();
          })
          .catch(err => console.error("âŒ Error al enviar datos visuales:", err));

          lastSent = now;
        }
      }

      canvasCtx.restore();
    });

    const camera = new Camera(videoElement, {
      onFrame: async () => {
        await faceMesh.send({ image: videoElement });
      },
      width: 320,
      height: 240
    });
    camera.start();
  </script>
</body>
</html>
