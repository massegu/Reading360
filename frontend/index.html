<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <title>Reading 360</title>
  <style>
    /* Aqu√≠ va tu CSS */
 /* Estilos generales */
body {
  font-family: Arial, sans-serif;
  margin: 0;
  padding: 20px;
  display: flex;
  flex-direction: column;
  align-items: center;
  text-align: left;
}

/* Layout principal: dos columnas */
.main-layout {
  display: grid;
  grid-template-columns: 180px 1fr;
  gap: 40px;
  align-items: center; /* Alinea verticalmente audio y t√≠tulo */
  width: 100%;
  max-width: 1200px;
  margin: 0 auto;
  padding-top: 20px;
}

/* Panel izquierdo: audio + botones */
.controls-panel {
  display: flex;
  flex-direction: column;
  gap: 20px;
  align-items: flex-start;
}

.audio-wrapper {
  margin-bottom: 10px;
  width: 100%;
}

.controls {
  display: flex;
  flex-direction: row;
  gap: 15px;
  margin-bottom: 20px;
}

/* Panel derecho: t√≠tulo alineado con audio */
.header-aligned {
  display: flex;
  align-items: center;
  height: 100%;
}

.header-aligned h1 {
  margin: 0;
  font-size: 32px;
}
.header-row {
  display: flex;
  justify-content: flex-start;
  align-items: center;
  gap: 350px;
  width: 100%;
  max-width: 800px;
  margin-bottom: 20px;
}

.header-row h1 {
  margin: 0;
  font-size: 32px;
}

.header-row audio {
  flex-shrink: 0;
  width: 200px;
}

/* Panel de contenido debajo del t√≠tulo */
.content-panel {
  display: flex;
  flex-direction: column;
  gap: 20px;
  width: 100%;
  max-width: 800px;
  margin-top: 30px;
}

/* Selector de nivel y explicaci√≥n */
.level-row {
  display: flex;
  flex-wrap: wrap;
  justify-content: space-between;
  align-items: center;
  gap: 20px;
  margin-bottom: 20px;
}

.instructions {
  flex: 1;
  font-size: 16px;
  margin: 0;
}

.level-selector {
  display: flex;
  align-items: center;
  gap: 10px;
}

/* Texto a leer */
#readingText {
  background-color: #fefefe;
  border: 2px solid #ccc;
  padding: 16px;
  font-size: 30px;
  line-height: 1.6;
  width: 100%;
  box-shadow: 0 0 8px rgba(0,0,0,0.1);
  text-align: left;
}

/* V√≠deo y canvas */
.video-container {
  position: relative;
  width: 640px;
  height: 480px;
  margin-top: 20px;
}

#video {
  border: 1px solid #ccc;
  width: 640px;
  height: 480px;
}

#output {
  position: absolute;
  left: 0;
  top: 0;
  width: 640px;
  height: 480px;
  pointer-events: none;
  background-color: transparent;
}

/* Estado de la mirada */
#gazeStatus {
  font-size: 18px;
  margin-top: 10px;
  text-align: center;
  width: 80%;
}

/* Adaptaci√≥n m√≥vil */
@media (max-width: 600px) {
  .main-layout {
    display: flex;
    flex-direction: column;
    align-items: center;
  }

  .controls {
    flex-direction: row;
    gap: 10px;
  }

  .video-container {
    width: 100%;
    height: auto;
  }

  #video, #output {
    width: 100%;
    height: auto;
  }

  #readingText {
    font-size: 18px;
  }

  #gazeStatus {
    font-size: 14px;
    margin-top: 10px;
  }
}


@media (max-width: 600px) {
  .video-container {
    width: 100%;
    height: auto;
  }

  #video, #output {
    width: 100%;
    height: auto;
  }

  #readingText {
    font-size: 18px;
  }
}

  </style>
</head>
<body>
<!-- Encabezado con t√≠tulo y reproductor alineados horizontalmente -->
<div class="header-row">
  <h1>üéôÔ∏è Reading 360</h1>
  <audio id="audioPlayer" controls></audio>
</div>

<!-- Botones de grabaci√≥n en l√≠nea horizontal -->
<div class="controls">
  <button id="startBtn">üî¥ Grabar</button>
  <button id="stopBtn" disabled>‚èπÔ∏è Detener</button>
  <button id="sendBtn" disabled>üì§ Enviar</button>
</div>

<!-- Panel de contenido -->
<div class="content-panel">
  <div class="level-row">
    <p class="instructions">Selecciona un nivel, lee el texto en voz alta y luego env√≠a el audio para analizarlo.</p>
  </div>

  <div class="level-selector">
    <label for="levelSelect">üìö Nivel de lectura:</label>
    <select id="levelSelect">
      <option value="easy">Nivel 1 ‚Äì F√°cil</option>
      <option value="medium">Nivel 2 ‚Äì Intermedio</option>
      <option value="hard">Nivel 3 ‚Äì Avanzado</option>
    </select>
  </div>

  <div id="readingText"></div>

  <div class="video-container">
    <video id="video" autoplay muted playsinline width="640" height="480"></video>
    <canvas id="output" width="640" height="480"></canvas>
  </div>

  <p id="gazeStatus">üëÅÔ∏è Direcci√≥n de la mirada: desconocida</p>
</div>

   
  

  <!-- Scripts MediaPipe -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

  <script>
    // Textos por nivel
    const texts = {
      easy: "El gato duerme en la cama. La ni√±a lee un libro. El sol brilla en el cielo.",
      medium: "Cada ma√±ana, Marcos se levanta temprano para ir al colegio. Se lava la cara, desayuna con su familia y sale con su mochila llena de libros.",
      hard: "Aunque el cielo estaba cubierto de nubes, Clara decidi√≥ salir a caminar por el bosque. El aire era fresco, las hojas cruj√≠an bajo sus pies y el silencio la envolv√≠a como una manta suave."
    };

    const levelSelect = document.getElementById("levelSelect");
    const readingText = document.getElementById("readingText");
    readingText.innerText = texts[levelSelect.value];
    levelSelect.onchange = () => {
      readingText.innerText = texts[levelSelect.value];
    };

    // Grabaci√≥n de voz
    let mediaRecorder;
    let audioChunks = [];
    let latestVoiceMetrics = null;
    let latestVisualMetrics = null;
    let readingRegistered = false;
    const sessionId = Date.now().toString();

    const startBtn = document.getElementById("startBtn");
    const stopBtn = document.getElementById("stopBtn");
    const sendBtn = document.getElementById("sendBtn");
    const audioPlayback = document.getElementById("audioPlayer");

    startBtn.onclick = async () => {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      audioChunks = [];

      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
        audioPlayback.src = URL.createObjectURL(audioBlob);
        sendBtn.disabled = false;
      };

      mediaRecorder.start();
      startBtn.disabled = true;
      stopBtn.disabled = false;
    };

    stopBtn.onclick = () => {
      mediaRecorder.stop();
      startBtn.disabled = false;
      stopBtn.disabled = true;
    };

    sendBtn.onclick = async () => {
      const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
      const formData = new FormData();
      formData.append("audio", audioBlob, "lectura.wav");

      const response = await fetch("http://localhost:5000/upload-audio", {
        method: "POST",
        body: formData
      });

      const voiceResult = await response.json();
      latestVoiceMetrics = voiceResult;
      alert("‚úÖ Voz analizada:\n" + JSON.stringify(voiceResult, null, 2));
      tryRegisterReading();
    };

    async function tryRegisterReading() {
      if (readingRegistered) return;
      if (!latestVoiceMetrics || !latestVisualMetrics) {
        console.log("‚è≥ Esperando ambos an√°lisis para registrar...");
        return;
      }

      const readingData = {
        session_id: sessionId,
        user_id: "demo_user",
        text_id: levelSelect.value,
        label: "unlabeled",
        voice: latestVoiceMetrics,
        visual: latestVisualMetrics
      };

      const res = await fetch("http://localhost:5000/register-reading", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(readingData)
      });

      const result = await res.json();
      alert("üìö Lectura registrada:\n" + JSON.stringify(result, null, 2));
      readingRegistered = true;
    }

    // Seguimiento facial refinado
    const videoElement = document.getElementById('video');
    const canvasElement = document.getElementById('output');
    const canvasCtx = canvasElement.getContext('2d');

    const faceMesh = new FaceMesh({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
    });

    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });

    const leftEyeIndices = FACEMESH_LEFT_EYE.map(pair => pair[0]);
    const rightEyeIndices = FACEMESH_RIGHT_EYE.map(pair => pair[0]);

    let lastSent = 0;

    function averagePoint(points) {
      const sum = points.reduce((acc, p) => ({
        x: acc.x + p.x,
        y: acc.y + p.y
      }), { x: 0, y: 0 });

      return {
        x: sum.x / points.length,
        y: sum.y / points.length
      };
    }

    faceMesh.onResults(results => {
  const now = Date.now();
  if (now - lastSent < 2000) return;

  canvasCtx.save();
  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
  canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);

  if (results.multiFaceLandmarks) {
    for (const landmarks of results.multiFaceLandmarks) {
      // Dibujar malla facial
      drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION, { color: '#C0C0C0', lineWidth: 1 });
      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYE, { color: '#FF3030' });
      drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_EYE, { color: '#30FF30' });

      // C√°lculo de direcci√≥n de la mirada usando iris derecho (cuanto m√°s cerca de 0.45 = izquierda, 0.65 = derecha)
      let gazeDirection = "centro";
      try {
        const leftCorner = landmarks[33];   // borde izquierdo del ojo derecho
        const rightCorner = landmarks[133]; // borde derecho del ojo derecho
        const pupil = landmarks[468];       // centro estimado del iris derecho

        const eyeWidth = rightCorner.x - leftCorner.x;
        const pupilOffset = pupil.x - leftCorner.x;
        const ratio = 1 - pupilOffset / eyeWidth;

        if (ratio < 0.35) gazeDirection = "izquierda";
        else if (ratio > 0.55) gazeDirection = "derecha";
        else gazeDirection = "centro"; 
      } catch (e) {
        console.warn("No se pudo calcular la direcci√≥n de la mirada con el iris.");
      }

      // C√°lculo de centros de ojos (para an√°lisis visual adicional)
      const leftEyePoints = leftEyeIndices.map(i => landmarks[i]);
      const rightEyePoints = rightEyeIndices.map(i => landmarks[i]);

      const leftEyeCenter = averagePoint(leftEyePoints);
      const rightEyeCenter = averagePoint(rightEyePoints);

      // Mostrar direcci√≥n en pantalla
      document.getElementById("gazeStatus").innerText = "üëÅÔ∏è Direcci√≥n de la mirada: " + gazeDirection;

      // Enviar datos visuales al backend
      fetch("http://localhost:5000/upload-visual", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          leftEyeCenter,
          rightEyeCenter,
          gazeDirection
        })
      })
      .then(res => res.json())
      .then(data => {
        latestVisualMetrics = data;
        console.log("‚úÖ Atenci√≥n visual:", data);
        tryRegisterReading();
      })
      .catch(err => console.error("‚ùå Error al enviar datos visuales:", err));
    }

    lastSent = now;
  }

  canvasCtx.restore();
});


    const camera = new Camera(videoElement, {
      onFrame: async () => {
        await faceMesh.send({ image: videoElement });
      },
      width: 320,
      height: 240
    });
    camera.start();
  </script>
</body>
</html>
