<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <title>Reading360 - Grabaci√≥n de voz</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0px;
      padding: 20px;
      display: flex;
      flex-direction: column;
      align-items: center;
      text-align: center;
    }

    h1, h2, h3 {
      margin-top: 30px;
    }

  .level-selector {
  display: flex;
  align-items: center;
  gap: 10px;
  flex-wrap: wrap;
  justify-content: center;
  margin-top: 20px;
  max-width: 800px;
  text-align: left;
}

  .reading-visual-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 20px;
  margin-top: 20px;
}

#readingText {
  text-align: left;
  max-width: 600px;
  width: 100%;
  margin: 0 auto;
  font-size: 30px;
}

.controls {
  position: absolute;
  left: 20px;
  top: 40px;
  display: flex;
  flex-direction: column;
  gap: 10px;
}

    audio {
      margin-top: 10px;
    }

    .video-container {
      position: relative;
      width: 320px;
      height: 240px;
      margin-top: 20px;
    }

    #video {
      border: 1px solid #ccc;
    }
    #output {
      position: absolute;
      left: 0;
      top: 0;
      width: 320px;
      height: 240px;
      pointer-events: none;
      background-color: transparent;
  }

  @media (max-width: 600px) {
  .video-container {
    width: 100%;
    height: auto;
  }

  #video, #output {
    width: 100%;
    height: auto;
  }

  #readingText {
    font-size: 18px;
  }
}

  </style>
</head>
<body>
  <body>
  <h1>üéôÔ∏è Reading360</h1>
  <!-- Selector de nivel -->
 <div class="level-selector">
  <span>Selecciona un nivel, lee el texto en voz alta y luego env√≠a el audio para analizarlo. üìö Nivel de lectura:</span>
  <select id="levelSelect">
    <option value="easy">Nivel 1 ‚Äì F√°cil</option>
    <option value="medium">Nivel 2 ‚Äì Intermedio</option>
    <option value="hard">Nivel 3 ‚Äì Avanzado</option>
  </select>
</div>

  <!-- Contenedor principal: texto + c√°mara -->
 <div class="reading-visual-container">
  <div id="readingText"></div>

  <div class="video-container">
    <video id="video" autoplay muted playsinline width="320" height="240"></video>
    <canvas id="output" width="320" height="240"></canvas>
  </div>

  <p id="gazeStatus">üëÅÔ∏è Direcci√≥n de la mirada: desconocida</p>
</div>


  <!-- Botones de grabaci√≥n en lateral -->
  <div class="controls">
    <button id="startBtn">üî¥ Grabar</button>
    <button id="stopBtn" disabled>‚èπÔ∏è Detener</button>
    <button id="sendBtn" disabled>üì§ Enviar</button>
  </div>

  <audio id="audioPlayback" controls></audio>

  <!-- Scripts MediaPipe -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

  <script>
    // Textos por nivel
    const texts = {
      easy: "El gato duerme en la cama. La ni√±a lee un libro. El sol brilla en el cielo.",
      medium: "Cada ma√±ana, Marcos se levanta temprano para ir al colegio. Se lava la cara, desayuna con su familia y sale con su mochila llena de libros.",
      hard: "Aunque el cielo estaba cubierto de nubes, Clara decidi√≥ salir a caminar por el bosque. El aire era fresco, las hojas cruj√≠an bajo sus pies y el silencio la envolv√≠a como una manta suave."
    };

    const levelSelect = document.getElementById("levelSelect");
    const readingText = document.getElementById("readingText");
    readingText.innerText = texts[levelSelect.value];
    levelSelect.onchange = () => {
      readingText.innerText = texts[levelSelect.value];
    };

    // Grabaci√≥n de voz
    let mediaRecorder;
    let audioChunks = [];
    let latestVoiceMetrics = null;
    let latestVisualMetrics = null;
    let readingRegistered = false;
    const sessionId = Date.now().toString();

    const startBtn = document.getElementById("startBtn");
    const stopBtn = document.getElementById("stopBtn");
    const sendBtn = document.getElementById("sendBtn");
    const audioPlayback = document.getElementById("audioPlayback");

    startBtn.onclick = async () => {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      audioChunks = [];

      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
        audioPlayback.src = URL.createObjectURL(audioBlob);
        sendBtn.disabled = false;
      };

      mediaRecorder.start();
      startBtn.disabled = true;
      stopBtn.disabled = false;
    };

    stopBtn.onclick = () => {
      mediaRecorder.stop();
      startBtn.disabled = false;
      stopBtn.disabled = true;
    };

    sendBtn.onclick = async () => {
      const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
      const formData = new FormData();
      formData.append("audio", audioBlob, "lectura.wav");

      const response = await fetch("http://localhost:5000/upload-audio", {
        method: "POST",
        body: formData
      });

      const voiceResult = await response.json();
      latestVoiceMetrics = voiceResult;
      alert("‚úÖ Voz analizada:\n" + JSON.stringify(voiceResult, null, 2));
      tryRegisterReading();
    };

    async function tryRegisterReading() {
      if (readingRegistered) return;
      if (!latestVoiceMetrics || !latestVisualMetrics) {
        console.log("‚è≥ Esperando ambos an√°lisis para registrar...");
        return;
      }

      const readingData = {
        session_id: sessionId,
        user_id: "demo_user",
        text_id: levelSelect.value,
        label: "unlabeled",
        voice: latestVoiceMetrics,
        visual: latestVisualMetrics
      };

      const res = await fetch("http://localhost:5000/register-reading", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(readingData)
      });

      const result = await res.json();
      alert("üìö Lectura registrada:\n" + JSON.stringify(result, null, 2));
      readingRegistered = true;
    }

    // Seguimiento facial refinado
    const videoElement = document.getElementById('video');
    const canvasElement = document.getElementById('output');
    const canvasCtx = canvasElement.getContext('2d');

    const faceMesh = new FaceMesh({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
    });

    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });

    const leftEyeIndices = FACEMESH_LEFT_EYE.map(pair => pair[0]);
    const rightEyeIndices = FACEMESH_RIGHT_EYE.map(pair => pair[0]);

    let lastSent = 0;

    function averagePoint(points) {
      const sum = points.reduce((acc, p) => ({
        x: acc.x + p.x,
        y: acc.y + p.y
      }), { x: 0, y: 0 });

      return {
        x: sum.x / points.length,
        y: sum.y / points.length
      };
    }

    faceMesh.onResults(results => {
  const now = Date.now();
  if (now - lastSent < 2000) return;

  canvasCtx.save();
  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
  canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);

  if (results.multiFaceLandmarks) {
    for (const landmarks of results.multiFaceLandmarks) {
      // Dibujar malla facial
      drawConnectors(canvasCtx, landmarks, FACEMESH_TESSELATION, { color: '#C0C0C0', lineWidth: 1 });
      drawConnectors(canvasCtx, landmarks, FACEMESH_RIGHT_EYE, { color: '#FF3030' });
      drawConnectors(canvasCtx, landmarks, FACEMESH_LEFT_EYE, { color: '#30FF30' });

      // C√°lculo de direcci√≥n de la mirada usando iris derecho (cuanto m√°s cerca de 0.45 = izquierda, 0.65 = derecha)
      let gazeDirection = "centro";
      try {
        const leftCorner = landmarks[33];   // borde izquierdo del ojo derecho
        const rightCorner = landmarks[133]; // borde derecho del ojo derecho
        const pupil = landmarks[468];       // centro estimado del iris derecho

        const eyeWidth = rightCorner.x - leftCorner.x;
        const pupilOffset = pupil.x - leftCorner.x;
        const ratio = 1 - pupilOffset / eyeWidth;

        if (ratio < 0.35) gazeDirection = "izquierda";
        else if (ratio > 0.55) gazeDirection = "derecha";
        else gazeDirection = "centro"; 
      } catch (e) {
        console.warn("No se pudo calcular la direcci√≥n de la mirada con el iris.");
      }

      // C√°lculo de centros de ojos (para an√°lisis visual adicional)
      const leftEyePoints = leftEyeIndices.map(i => landmarks[i]);
      const rightEyePoints = rightEyeIndices.map(i => landmarks[i]);

      const leftEyeCenter = averagePoint(leftEyePoints);
      const rightEyeCenter = averagePoint(rightEyePoints);

      // Mostrar direcci√≥n en pantalla
      document.getElementById("gazeStatus").innerText = "üëÅÔ∏è Direcci√≥n de la mirada: " + gazeDirection;

      // Enviar datos visuales al backend
      fetch("http://localhost:5000/upload-visual", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          leftEyeCenter,
          rightEyeCenter,
          gazeDirection
        })
      })
      .then(res => res.json())
      .then(data => {
        latestVisualMetrics = data;
        console.log("‚úÖ Atenci√≥n visual:", data);
        tryRegisterReading();
      })
      .catch(err => console.error("‚ùå Error al enviar datos visuales:", err));
    }

    lastSent = now;
  }

  canvasCtx.restore();
});


    const camera = new Camera(videoElement, {
      onFrame: async () => {
        await faceMesh.send({ image: videoElement });
      },
      width: 320,
      height: 240
    });
    camera.start();
  </script>
</body>
</html>
